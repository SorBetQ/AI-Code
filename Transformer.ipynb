{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3d0861",
   "metadata": {},
   "source": [
    "# Embedding层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e1622",
   "metadata": {},
   "source": [
    "## Tocken Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TockenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size) #vocab_size 表示词汇表中词元的数量，hidden_size 表示每个词元被映射到的高维空间的维度（即嵌入维度）\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        return embedded\n",
    "\n",
    "def test_tocken_embedding():\n",
    "    vocab_size = 10000 #词汇表大小\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 512\n",
    "    # torch.randint 生成随机整数张量,生成的整数会在 [0, vocab_size - 1] 这个区间内,生成的张量的形状是 (batch_size, seq_len)\n",
    "    # torch.randn 生成的是从标准正态分布（均值为 0，方差为 1）中采样的随机浮点数\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    tocken_embedding = TockenEmbedding(vocab_size, hidden_size)\n",
    "\n",
    "    output = tocken_embedding(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tocken_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789cc77",
   "metadata": {},
   "source": [
    "## Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float() # shape:(max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (- math.log(10000.0) / hidden_size)) # shape:(1, hidden_size/2)\n",
    "\n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # shape:(max_len, hidden_size)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:seq_len, :].unsqueeze(0) # shape:(1, seq_len, hidden_size)\n",
    "        return x\n",
    "    \n",
    "def test_positional_embedding():\n",
    "    max_len = 5000\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 512\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    positional_embedding = PositionalEmbedding(max_len, hidden_size)\n",
    "    output = positional_embedding(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_positional_embedding()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac28e65",
   "metadata": {},
   "source": [
    "# Encoder 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, ff_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, ff_size)\n",
    "        )\n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        # mha子层\n",
    "        attn_output = self.multi_head_attention(x, attention_mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layer_norm1(x + attn_output) #残差连接\n",
    "\n",
    "        # feed_forward 子层\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        ff_output = self.dropout2(ff_output)\n",
    "        out2 = self.layer_norm2(out1 + ff_output) # 残差连接\n",
    "\n",
    "        return out2\n",
    "    \n",
    "def main():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    ff_size = 2048\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    encoder_layer = EncoderLayer(hidden_size, num_heads, ff_size)\n",
    "\n",
    "    output = encoder_layer(x)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc32cb",
   "metadata": {},
   "source": [
    "# Decoder 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attenion = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, ff_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, ff_size)\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm3 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_attention_mask = None, encoder_attention_mask = None):\n",
    "        # self_attention\n",
    "        self_attn_output = self.self_attenion(x, self_attention_mask)\n",
    "        self_attn_output = self.dropout1(self_attn_output)\n",
    "        out1 = self.layer_norm1(x + self_attn_output)\n",
    "\n",
    "        # enc_dec_attn\n",
    "        enc_dec_attn_output = self.encoder_decoder_attention(out1, encoder_output, encoder_attention_mask)\n",
    "        enc_dec_attn_output = self.dropout2(enc_dec_attn_output)\n",
    "        out2 = self.layer_norm2(out1 + enc_dec_attn_output)\n",
    "\n",
    "        # ff\n",
    "        ff_output = self.feed_forward(out2)\n",
    "        ff_output = self.dropout3(ff_output)\n",
    "        out3 = self.layer_norm3(out2 + ff_output)\n",
    "\n",
    "        return out3\n",
    "    \n",
    "def main():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    ff_size = 2048\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    encoder_output = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    encoder_layer = EncoderLayer(hidden_size, num_heads, ff_size)\n",
    "\n",
    "    output = encoder_layer(x, encoder_output)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Encoder output shape:\", encoder_output.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea249aa",
   "metadata": {},
   "source": [
    "# 堆叠 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size, num_layers, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hidden_size, num_heads, ff_size, dropout_prob)\n",
    "            for _ in range(num_layers) \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def main():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    ff_size = 2048\n",
    "    num_layers = 6\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    encoder = Encoder(hidden_size, num_heads, ff_size, num_layers)\n",
    "\n",
    "    output = encoder(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521ecd0",
   "metadata": {},
   "source": [
    "# 堆叠 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size, num_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hidden_size, num_heads, ff_size, dropout_prob)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_output, self_attention_mask = None, encoder_decoder_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, self_attention_mask, encoder_decoder_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def main():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    ff_size = 2048\n",
    "    num_layers = 6\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    encoder_output = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    decoder = Decoder(hidden_size, num_heads, ff_size, num_layers)\n",
    "\n",
    "    output = decoder(x, encoder_output)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Encoder output shape:\", encoder_output.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee1163",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e62943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_heads, ff_size, num_layers, max_seq_len, dropout_prob = 0.1):\n",
    "        self.token_embedding = TockenEmbedding(vocab_size, hidden_size)\n",
    "        self.positional_embedding = PositionalEmbedding(hidden_size, max_seq_len)\n",
    "\n",
    "        self.encoder = Encoder(hidden_size, num_heads, ff_size, num_layers, dropout_prob)\n",
    "        self.decoder = Decoder(hidden_size, num_heads, ff_size, num_layers, dropout_prob)\n",
    "\n",
    "        self.output_linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, src_tgt_mask = None):\n",
    "        src_emb = self.token_embedding(src) + self.positional_embedding(src)\n",
    "        tgt_emb = self.token_embedding(tgt) + self.positional_embedding(tgt)\n",
    "\n",
    "        encoder_output = self.encoder(src_emb, src_mask)\n",
    "        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask, src_tgt_mask)\n",
    "\n",
    "        output = self.output_linear(decoder_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def main():\n",
    "    vocab_size = 1000\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    ff_size = 2048\n",
    "    num_layers = 6\n",
    "    max_seq_len = 100\n",
    "    dropout_prob = 0.1\n",
    "    batch_size = 2\n",
    "    src_seq_len = 10\n",
    "    tgt_seq_len = 10\n",
    "\n",
    "    src = torch.randint(0, vocab_size, (batch_size, src_seq_len))\n",
    "    tgt = torch.randint(0, vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "    transformer = Transformer(vocab_size, hidden_size, num_heads, ff_size, num_layers, max_seq_len, dropout_prob)\n",
    "\n",
    "    output = transformer(src, tgt)\n",
    "\n",
    "    print(\"Source shape:\", src.shape)\n",
    "    print(\"Target shape:\", tgt.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
