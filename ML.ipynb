{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3d9ff8",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be84cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate = 0.1, n_iters = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a76aa8",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be80c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.1, n_iters = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(- z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(linear)\n",
    "    \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        return (self.predict_prob(X) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba75d8d",
   "metadata": {},
   "source": [
    "# Softmax 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftMaxRegression:\n",
    "    def __init__(self, n_classes, learning_rate = 0.01, n_iters = 1000):\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        exp_Z = np.exp(Z - max(Z, axis = 1, keepdim = True))\n",
    "        return exp_Z / exp_Z.sum(axis = 1, keepdim = True)\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        m = y.shape[0]\n",
    "        y_one_hot = np.zeros((m, self.n_classes))\n",
    "        y_one_hot[np.range(m), y] = 1\n",
    "        return y_one_hot\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_one_hot = self.one_hot(y)\n",
    "        self.weights = np.random((self.n_classes, n_features))\n",
    "        self.bias = np.random(self.n_classes)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            Z = np.dot(X, self.weights) + self.bias\n",
    "            P = self.softmax(Z)\n",
    "\n",
    "            dZ = P - y_one_hot\n",
    "            dw = (1 / n_samples) * np.dot(dZ.T, X)\n",
    "            db = (1 / n_samples) * np.dot(dZ, axis = 0)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Z = np.dot(X, self.weights) + self.bias\n",
    "        P = self.softmax(Z)\n",
    "        return np.max(P, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913543f",
   "metadata": {},
   "source": [
    "# SGD 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed059e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.3099\n",
      "Final validation loss: 0.3071\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LinearRegressionSGD:\n",
    "    def __init__(self, learning_rate = 0.001, n_iters = 1000, batch_size = 64, validate_every = 4):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.validate_every = validate_every\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "\n",
    "    def add_bias_term(self, X):\n",
    "        return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    \n",
    "    def get_batches(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(0, n_samples, self.batch_size):\n",
    "            end = min(i + self.batch_size, n_samples)\n",
    "            yield X[i : end], y[i : end]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return 0.5 * np.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    def compute_gradient(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = self.predict(X)\n",
    "        error = y_pred - y\n",
    "        dw = np.dot(X.T, error) / n_samples\n",
    "        db = np.mean(error)\n",
    "        return dw, db\n",
    "\n",
    "    def shuffled_data(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val = None, y_val = None):\n",
    "        X_train_bias = self.add_bias_term(X_train)\n",
    "        n_features = X_train_bias.shape[1]\n",
    "        n_samples = X_train.shape[0]\n",
    "        self.weights = np.random.randn(n_features - 1)\n",
    "        self.bias = 0\n",
    "\n",
    "        for epoch in range(self.n_iters):\n",
    "            epoch_loss = 0\n",
    "            X_shuffled, y_shuffled = self.shuffled_data(X_train_bias, y_train)\n",
    "\n",
    "            for X_batch, y_batch in self.get_batches(X_shuffled, y_shuffled):\n",
    "                y_pred = np.dot(X_batch[:, :-1], self.weights) + self.bias\n",
    "\n",
    "                batch_loss = 0.5 * np.mean((y_pred - y_batch) ** 2)\n",
    "                epoch_loss += batch_loss * X_batch.shape[0] / n_samples\n",
    "                dw, db = self.compute_gradient(X_batch[:, :-1], y_batch)\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            self.train_loss_history.append(epoch_loss)\n",
    "\n",
    "            if X_val is not None and y_val is not None and (epoch % self.validate_every) == 0:\n",
    "                X_val_bias = self.add_bias_term(X_val)\n",
    "                val_loss = self.compute_loss(X_val_bias[:, :-1], y_val)\n",
    "                self.val_loss_history.append(val_loss)\n",
    "    \n",
    "def generate_sample_data(n_samples = 2000, n_features = 5, noise_level = 0.8, random_seed = 9):\n",
    "    np.random.seed(random_seed)\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    true_weights = np.random.randn(n_features)\n",
    "    true_bias = np.random.randn()\n",
    "\n",
    "    y = np.dot(X, true_weights) + true_bias + noise_level * np.random.randn(n_samples)\n",
    "    return X, y, true_weights, true_bias\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X, y, true_weights, true_bias = generate_sample_data()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle= True)\n",
    "\n",
    "    model = LinearRegressionSGD()\n",
    "\n",
    "    model.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(f\"Final training loss: {model.train_loss_history[-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {model.val_loss_history[-1]:.4f}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa994336",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans(data, k, threshold = 1, max_iterations = 100):\n",
    "    centers = data[np.random.choice(data.shape[0], k, replace = False)]\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        distances = np.linalg.norm(data[:, None] - centers, axis= 2)\n",
    "        labels = np.argmin(distances, axis= 1)\n",
    "        new_centers = np.array([data[labels == i].mean(axis = 0) for i in range(k)])\n",
    "\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        \n",
    "        centers = new_centers\n",
    "    return labels, centers\n",
    "\n",
    "data = np.random.rand(100, 2)\n",
    "k = 3\n",
    "labels, centers = kmeans(data, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
