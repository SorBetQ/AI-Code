{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526151ed",
   "metadata": {},
   "source": [
    "# LN 层归一化（Layer Normalization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08965c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        variance = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "\n",
    "        x_normalized = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "\n",
    "        output = self.gamma * x_normalized + self.beta\n",
    "\n",
    "        return output\n",
    "\n",
    "def test_layer_norm():\n",
    "    batch_size = 2\n",
    "    seq_size = 4\n",
    "    hidden_size = 8\n",
    "\n",
    "    x = torch.randn(batch_size, seq_size, hidden_size)\n",
    "    \n",
    "    layer_norm = LayerNorm(hidden_size)\n",
    "\n",
    "    output = layer_norm(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_layer_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfaa4f3",
   "metadata": {},
   "source": [
    "# RMSNorm 均方根归一化（Root Mean Square）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x.pow(2), dim = -1, keepdim= True) + self.eps)\n",
    "\n",
    "        x_normalized = x // rms\n",
    "\n",
    "        output = self.gamma * x_normalized\n",
    "\n",
    "        return output\n",
    "    \n",
    "def test_rms_norm():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 8\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    rms_norm = RMSNorm(hidden_size)\n",
    "    \n",
    "    output = rms_norm(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Parameters:\", list(rms_norm.parameters()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_rms_norm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233983c7",
   "metadata": {},
   "source": [
    "# BN 批次归一化 (Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps = 1e-6, momentum = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.betta = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.running_mean = torch.zeros(hidden_size)\n",
    "        self.running_var = torch.ones(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.train:\n",
    "            batch_mean = x.mean(dim = (0, 1), keepdim = True)\n",
    "            batch_var = x.var(dim = (0, 1), keepdim = True)\n",
    "\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "            mean = batch_mean\n",
    "            variance = batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            variance = self.running_var\n",
    "        \n",
    "        x_normalized = (x - mean) // torch.sqrt(variance + self.eps)\n",
    "\n",
    "        output = self.gamma * x_normalized + self.betta\n",
    "\n",
    "        return output\n",
    "\n",
    "def test_batch_norm():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 8\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    bn = BatchNorm(hidden_size)\n",
    "\n",
    "    output = bn(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_batch_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f4162",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    def __init__(self, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.train:\n",
    "            mask = (torch.randn(x.shape) > self.dropout_prob).float()\n",
    "\n",
    "            output = mask * x / (1.0 - self.dropout_prob)\n",
    "        else:\n",
    "            output = x\n",
    "        \n",
    "        return output\n",
    "\n",
    "def test_dropout():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 8\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    dropout = Dropout(dropout_prob= 0.1)\n",
    "\n",
    "    dropout.train()\n",
    "    output_train = dropout(x)\n",
    "\n",
    "    dropout.eval()\n",
    "    output_eval = dropout(x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape during training:\", output_train.shape)\n",
    "    print(\"Output shape during evaluation:\", output_eval.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85969d",
   "metadata": {},
   "source": [
    "# Backpropagation 反向传播梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#define model, optimizer, criterion, dataloader\n",
    "model = MyModel()\n",
    "optimizer = optim.sgd(model.parameters(), lr = 0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dataloader = MyDataloader()\n",
    "\n",
    "model.tarin()\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739e085",
   "metadata": {},
   "source": [
    "### 反向传播 (BP) 算法核心公式详解\n",
    "\n",
    "#### 1. **前向传播公式**\n",
    "| 层类型 | 公式 | 说明 |\n",
    "|--------|------|------|\n",
    "| **输入层** | $a^{(0)} = x$ | $x$ 为输入数据 |\n",
    "| **隐藏层** | $z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$ | 线性变换 |\n",
    "| | $a^{(l)} = f(z^{(l)})$ | 激活函数 $f$ (如 Sigmoid, ReLU) |\n",
    "| **输出层** | $\\hat{y} = a^{(L)}$ | 最终预测值 |\n",
    "\n",
    "#### 2. **损失函数 (均方误差 MSE)**\n",
    "$$ \\mathcal{L} = \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2 $$\n",
    "- $N$: 样本数量\n",
    "- $\\hat{y}$: 预测值\n",
    "- $y$: 真实值\n",
    "\n",
    "#### 3. **反向传播核心公式 (链式法则)**\n",
    "##### 3.1 输出层梯度\n",
    "$$ \\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(L)}} = (\\hat{y} - y) \\odot f'(z^{(L)}) $$\n",
    "- $\\odot$: 逐元素乘法\n",
    "- $f'$: 激活函数的导数\n",
    "\n",
    "##### 3.2 隐藏层梯度 (递归计算)\n",
    "$$ \\delta^{(l)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(l)}} = \\left( (W^{(l+1)})^T \\delta^{(l+1)} \\right) \\odot f'(z^{(l)}) $$\n",
    "\n",
    "##### 3.3 参数梯度计算\n",
    "| 参数 | 梯度公式 | 维度说明 |\n",
    "|------|----------|---------|\n",
    "| **权重梯度** | $\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$ | $d^{(l)} \\times d^{(l-1)}$ |\n",
    "| **偏置梯度** | $\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} = \\delta^{(l)}$ | $d^{(l)} \\times 1$ |\n",
    "\n",
    "#### 4. **激活函数导数**\n",
    "| 激活函数 | $f(z)$ | $f'(z)$ | 特点 |\n",
    "|----------|--------|---------|------|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | $f(z)(1-f(z))$ | 易梯度消失 |\n",
    "| **ReLU** | $\\max(0,z)$ | $\\begin{cases}1 & z>0\\\\0 & z\\leq0\\end{cases}$ | 缓解梯度消失 |\n",
    "| **Tanh** | $\\frac{e^z-e^{-z}}{e^z+e^{-z}}$ | $1-f(z)^2$ | 中心化输出 |\n",
    "\n",
    "#### 5. **参数更新规则**\n",
    "$$ \\begin{aligned}\n",
    "W^{(l)} &\\leftarrow W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} \\\\\n",
    "b^{(l)} &\\leftarrow b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
    "\\end{aligned} $$\n",
    "- $\\eta$: 学习率 (learning rate)\n",
    "\n",
    "#### 6. **批量梯度计算**\n",
    "$$ \\nabla_W \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\nabla_W \\mathcal{L}_i $$\n",
    "- 对 $N$ 个样本的梯度求平均\n",
    "\n",
    "### 公式推导详解 (以Sigmoid激活为例)\n",
    "\n",
    "#### 1. 输出层梯度推导：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(L)} &= \\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}} \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(L)}} \\\\\n",
    "&= (\\hat{y} - y) \\cdot \\underbrace{\\frac{\\partial \\sigma(z^{(L)})}{\\partial z^{(L)}}}_{\\text{Sigmoid导数}} \\\\\n",
    "&= (\\hat{y} - y) \\odot \\sigma(z^{(L)}) \\odot (1 - \\sigma(z^{(L)}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### 2. 隐藏层梯度推导：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(l)} &= \\frac{\\partial \\mathcal{L}}{\\partial z^{(l)}} \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial z^{(l+1)}} \\cdot \\frac{\\partial z^{(l+1)}}{\\partial a^{(l)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\\\\n",
    "&= \\delta^{(l+1)} \\cdot \\frac{\\partial}{\\partial a^{(l)}} \\left( W^{(l+1)}a^{(l)} + b^{(l+1)} \\right) \\cdot f'(z^{(l)}) \\\\\n",
    "&= \\left( (W^{(l+1)})^T \\delta^{(l+1)} \\right) \\odot f'(z^{(l)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 反向传播算法流程总结\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "A[前向传播] --> B[计算损失]\n",
    "B --> C[计算输出层梯度 δ⁽ᴸ⁾]\n",
    "C --> D[递归计算隐藏层梯度 δ⁽ˡ⁾]\n",
    "D --> E[计算权重梯度 ∂L/∂W]\n",
    "E --> F[计算偏置梯度 ∂L/∂b]\n",
    "F --> G[更新参数]\n",
    "G --> A\n",
    "```\n",
    "\n",
    "### 关键记忆点\n",
    "1. **误差反向传播**：$\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot f'(z^{(l)})$\n",
    "2. **权重梯度**：$\\nabla_W = \\delta^{(l)} (a^{(l-1)})^T$\n",
    "3. **链式法则**：核心数学原理\n",
    "4. **激活函数导数**：决定梯度传播特性\n",
    "5. **批量平均**：对多个样本梯度求平均\n",
    "\n",
    "> BP算法通过高效计算梯度，使深度神经网络训练成为可能。理解这些核心公式是掌握深度学习优化的基础，尤其要注意激活函数导数的计算，它直接影响梯度传播的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f380eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_dim: List[int], learning_rate: float = 0.3):\n",
    "        \"\"\"\n",
    "        初始化神经网络\n",
    "        \n",
    "        参数:\n",
    "        layers_dim -- 每层的神经元数量，例如 [1, 25, 1] 表示输入层1个神经元，隐藏层25个，输出层1个\n",
    "        learning_rate -- 学习率\n",
    "        \"\"\"\n",
    "        self.layers_dim = layers_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(layers_dim)\n",
    "        self.parameters = self._init_parameters()\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def _init_parameters(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"初始化权重和偏置\"\"\"\n",
    "        parameters = {}\n",
    "        # 使用Xavier/Glorot初始化权重，有助于改善训练\n",
    "        for i in range(1, self.num_layers):\n",
    "            # 输入维度\n",
    "            n_in = self.layers_dim[i-1]\n",
    "            # 输出维度\n",
    "            n_out = self.layers_dim[i]\n",
    "            \n",
    "            # 权重初始化\n",
    "            limit = np.sqrt(6 / (n_in + n_out))\n",
    "            parameters[f\"w{i}\"] = np.random.uniform(-limit, limit, (n_out, n_in))\n",
    "            parameters[f\"b{i}\"] = np.zeros((n_out, 1))\n",
    "        return parameters\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid激活函数\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_prime(z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid函数的导数\"\"\"\n",
    "        s = NeuralNetwork.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> Tuple[Dict[str, np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        返回:\n",
    "        caches -- 包含各层的z和a值的字典\n",
    "        output -- 网络输出\n",
    "        \"\"\"\n",
    "        # 确保输入是二维数组 (features, samples)\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "            \n",
    "        caches = {\"z\": [x], \"a\": [x]}  # 输入层\n",
    "        a = x\n",
    "        \n",
    "        # 隐藏层的前向传播 (使用sigmoid激活)\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            z = self.parameters[f\"w{i}\"].dot(a) + self.parameters[f\"b{i}\"]\n",
    "            a = self.sigmoid(z)\n",
    "            caches[\"z\"].append(z)\n",
    "            caches[\"a\"].append(a)\n",
    "        \n",
    "        # 输出层 (无激活函数)\n",
    "        z_output = self.parameters[f\"w{self.num_layers-1}\"].dot(a) + self.parameters[f\"b{self.num_layers-1}\"]\n",
    "        a_output = z_output\n",
    "        caches[\"z\"].append(z_output)\n",
    "        caches[\"a\"].append(a_output)\n",
    "        \n",
    "        return caches, a_output\n",
    "    \n",
    "    def backward(self, caches: Dict[str, np.ndarray], output: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        反向传播计算梯度\n",
    "        \n",
    "        返回:\n",
    "        gradients -- 包含各层权重和偏置梯度的字典\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        m = y.shape[1]  # 样本数量\n",
    "        \n",
    "        # 输出层梯度 (无激活函数)\n",
    "        dz_last = output - y\n",
    "        gradients[f\"dz{self.num_layers-1}\"] = dz_last\n",
    "        gradients[f\"dw{self.num_layers-1}\"] = dz_last.dot(caches[\"a\"][self.num_layers-2].T) / m\n",
    "        gradients[f\"db{self.num_layers-1}\"] = np.sum(dz_last, axis=1, keepdims=True) / m\n",
    "        \n",
    "        # 隐藏层梯度 (使用sigmoid激活)\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            dz = self.parameters[f\"w{i+1}\"].T.dot(gradients[f\"dz{i+1}\"]) * self.sigmoid_prime(caches[\"z\"][i])\n",
    "            gradients[f\"dz{i}\"] = dz\n",
    "            gradients[f\"dw{i}\"] = dz.dot(caches[\"a\"][i-1].T) / m\n",
    "            gradients[f\"db{i}\"] = np.sum(dz, axis=1, keepdims=True) / m\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients: Dict[str, np.ndarray]):\n",
    "        \"\"\"更新权重和偏置\"\"\"\n",
    "        for i in range(1, self.num_layers):\n",
    "            self.parameters[f\"w{i}\"] -= self.learning_rate * gradients[f\"dw{i}\"]\n",
    "            self.parameters[f\"b{i}\"] -= self.learning_rate * gradients[f\"db{i}\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_loss(output: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"计算均方误差损失\"\"\"\n",
    "        return np.mean(np.square(output - y))\n",
    "    \n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"预测输出\"\"\"\n",
    "        _, output = self.forward(x)\n",
    "        return output\n",
    "    \n",
    "    def train(self, x: np.ndarray, y: np.ndarray, epochs: int, log_interval: int = 100):\n",
    "        \"\"\"\n",
    "        训练神经网络\n",
    "        \n",
    "        参数:\n",
    "        x -- 输入数据 (1, n)\n",
    "        y -- 目标值 (1, n)\n",
    "        epochs -- 训练轮数\n",
    "        log_interval -- 打印损失的时间间隔\n",
    "        \"\"\"\n",
    "        # 确保数据格式正确\n",
    "        x = x.reshape(1, -1) if x.ndim == 1 else x\n",
    "        y = y.reshape(1, -1) if y.ndim == 1 else y\n",
    "        \n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # 前向传播\n",
    "            caches, output = self.forward(x)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = self.compute_loss(output, y)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # 反向传播\n",
    "            gradients = self.backward(caches, output, y)\n",
    "            \n",
    "            # 更新参数\n",
    "            self.update_parameters(gradients)\n",
    "            \n",
    "            # 定期打印训练进度\n",
    "            if epoch % log_interval == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:4d}/{epochs} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"绘制训练损失曲线\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.title(\"Training Loss History\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Mean Squared Error\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def generate_sine_data(num_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    生成正弦波数据\n",
    "    \n",
    "    返回:\n",
    "    x -- 输入数据 (1, num_samples)\n",
    "    y -- 目标值 (1, num_samples)\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, num_samples)\n",
    "    y = 20 * np.sin(2 * np.pi * x)\n",
    "    return x.reshape(1, -1), y.reshape(1, -1)\n",
    "\n",
    "\n",
    "def plot_results(x: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"绘制原始数据和预测结果\"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # 原始数据\n",
    "    plt.scatter(x.flatten(), y_true.flatten(), \n",
    "                color='blue', label='True Values', alpha=0.6)\n",
    "    \n",
    "    # 预测结果\n",
    "    plt.plot(x.flatten(), y_pred.flatten(), \n",
    "             color='red', linewidth=2, label='Predictions')\n",
    "    \n",
    "    plt.title(\"Neural Network Regression: True vs Predicted\")\n",
    "    plt.xlabel(\"Input\")\n",
    "    plt.ylabel(\"Output\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 配置参数\n",
    "    LAYERS_DIM = [1, 25, 1]  # 输入层1个神经元，隐藏层25个，输出层1个\n",
    "    LEARNING_RATE = 0.3\n",
    "    EPOCHS = 4000\n",
    "    LOG_INTERVAL = 500\n",
    "    \n",
    "    # 生成数据\n",
    "    x, y = generate_sine_data()\n",
    "    \n",
    "    # 初始化神经网络\n",
    "    print(\"Initializing neural network...\")\n",
    "    nn = NeuralNetwork(layers_dim=LAYERS_DIM, learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    # 训练神经网络\n",
    "    print(f\"Training neural network for {EPOCHS} epochs...\")\n",
    "    nn.train(x, y, epochs=EPOCHS, log_interval=LOG_INTERVAL)\n",
    "    \n",
    "    # 可视化训练过程\n",
    "    print(\"Plotting training history...\")\n",
    "    nn.plot_training_history()\n",
    "    \n",
    "    # 预测结果\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = nn.predict(x)\n",
    "    \n",
    "    # 可视化结果\n",
    "    print(\"Plotting results...\")\n",
    "    plot_results(x, y, predictions)\n",
    "    \n",
    "    # 计算最终损失\n",
    "    final_loss = nn.compute_loss(predictions, y)\n",
    "    print(f\"\\nTraining completed. Final loss: {final_loss:.6f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee4aa9",
   "metadata": {},
   "source": [
    "# Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 4\n",
    "\n",
    "for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "    preds = model(inputs)\n",
    "    loss = criterion(preds, labels)\n",
    "    \n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c226e68",
   "metadata": {},
   "source": [
    "# MLP 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "### 定义模型\n",
    "### N,1 -> N,10 -> N,10 -> N,1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.dense1 = nn.Linear(n_input, n_hidden)\n",
    "        self.dense2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "model = Net(1, 20, 1)\n",
    "print(model)\n",
    " \n",
    "### 准备数据\n",
    "x = torch.unsqueeze(torch.linspace(-1,1,100),dim=1)\n",
    "y = x.pow(3)+0.1*torch.randn(x.size())\n",
    " \n",
    "x , y =(Variable(x),Variable(y))\n",
    " \n",
    "plt.scatter(x.data,y.data)\n",
    "# 或者采用如下的方式也可以输出x,y\n",
    "# plt.scatter(x.data.numpy(),y.data.numpy())\n",
    "plt.show()\n",
    " \n",
    " \n",
    "####  pipeline\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    " \n",
    "for t in range(500):\n",
    "    predict = model(x)\n",
    "    loss = loss_func(predict, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t%5 ==0:\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), predict.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss = %.4f' % loss.data, fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.pause(0.05)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
