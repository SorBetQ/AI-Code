{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDPA 缩放点积注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, query, key, value, causal_mask = None, padding_mask = None):\n",
    "        # shape of q/k/v is (batch_size, seq_len, hidden_size)\n",
    "        # shape of attention_scores is (b, seq_len, seq_len)\n",
    "        # key needs to transpose\n",
    "        hidden_size = query.size(-1)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(hidden_size, dtype = torch.float32))\n",
    "\n",
    "        # shape of causal_mask is (b, seq_len,seq_len), \"1\" means where mask exits\n",
    "        if causal_mask is not None: # error use:'if causal_mask:'\n",
    "            attention_scores += causal_mask * -1e9\n",
    "\n",
    "        #padding_mask is (b, seq_len) -> (b, 1, 1, seq_len)\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            # PyTorch 的广播机制会将其视为 (batch_size, 1, seq_len)（自动对齐缺失维度）\n",
    "            attention_scores += padding_mask * -1e9\n",
    "        \n",
    "        attention_probs = torch.softmax(attention_scores, dim = -1) # shape:(b, num_heads, seq_len, seq_len)\n",
    "        attention_output = torch.matmul(attention_probs, value) # shape:(b, num_heads, seq_len, seq_len)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "def test_attn():\n",
    "    batch_size = 128\n",
    "    seq_len = 512\n",
    "    hidden_size = 1024\n",
    "\n",
    "    query = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    key = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    value = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "    sdpa = ScaledDotProductAttention()\n",
    "    output = sdpa(query, key, value)\n",
    "\n",
    "    print(\"Query shape:\", query.shape)\n",
    "    print(\"Key shape:\", key.shape)\n",
    "    print(\"Value shape:\", value.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output value:\", output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_attn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHA 多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, causal_mask = None, padding_mask = None):\n",
    "        # shape of hidden_size:(batch_size, seq_len, hidden_size)\n",
    "        batch_size = hidden_state.size(0)\n",
    "        # 为输入序列中计算q, k, v, 这是通过将输入词向量与三个权重矩阵相乘实现的\n",
    "        query = self.q_linear(hidden_state) # shape:(batch_size, seq_len, hidden_size)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)# shape:-> (b, seq_len, num_heads, head_dim) -> (b, num_heads, seq_len, head_dim)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype = torch.float32))# shape:(b, num_heads, seq_len, head_dim)\n",
    "\n",
    "        if causal_mask is not None:\n",
    "            attention_scores += causal_mask * -1e9\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_scores += padding_mask * -1e9\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim = -1)# shape:(b, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_probs, value)# shape:(b, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # 合并多头前确保内存连续\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "        output = output.view(batch_size, -1, self.head_dim * self.num_heads)# shape:(b, seq_len, hidden_size)\n",
    "\n",
    "        output = self.o_linear(output)# shape:(b, seq_len, hidden_size)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def test_MHA():\n",
    "    batch_size = 128\n",
    "    seq_len = 512\n",
    "    hidden_size = 1024\n",
    "    num_heads = 8\n",
    "\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    mha = MultiHeadAttention(hidden_size, num_heads)\n",
    "    output = mha(hidden_state, causal_mask = causal_mask)\n",
    "\n",
    "    print(\"Input shape:\", hidden_state.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_MHA()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHA with KV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, causal_mask = None, past_key_value = None, use_cache = False):\n",
    "        # shape of hidden_size:(batch_size, seq_len, hidden_size)\n",
    "        batch_size = hidden_state.size(0)\n",
    "        # 为输入序列中计算q, k, v, 这是通过将输入词向量与三个权重矩阵相乘实现的\n",
    "        query = self.q_linear(hidden_state) # shape:(batch_size, 1, hidden_size)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)# shape:-> (b, 1, num_heads, head_dim) -> (b, num_heads, 1, head_dim)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            past_key, past_value = past_key_value\n",
    "            key = torch.cat([past_key, key], dim = 2) # (b, num_heads, 1 + past_key.shape, head_dim)\n",
    "            value = torch.cat([past_value, value], dim = 2)\n",
    "\n",
    "        new_past_key_value = (key, value) if use_cache else None\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype = torch.float32))# shape:(b, num_heads, seq_len, head_dim)\n",
    "\n",
    "        if causal_mask is not None:\n",
    "            attention_scores += causal_mask * -1e9\n",
    "\n",
    "        attention_probs = torch.softmax(attention_scores, dim = -1)# shape:(b, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_probs, value)# shape:(b, num_heads, seq_len, head_dim)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)# shape:(b, seq_len, hidden_size)\n",
    "\n",
    "        output = self.o_linear(output)# shape:(b, seq_len, hidden_size)\n",
    "\n",
    "        return (output, new_past_key_value) if use_cache else output\n",
    "    \n",
    "def test_MHA_with_cache():\n",
    "    batch_size = 128\n",
    "    seq_len = 512\n",
    "    hidden_size = 1024\n",
    "    num_heads = 8\n",
    "\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    mha = MultiHeadAttention(hidden_size, num_heads)\n",
    "\n",
    "    past_key_value = None\n",
    "    outputs = []\n",
    "    for i in range(seq_len):\n",
    "        current_input = hidden_state[:, i:i+1, :]\n",
    "        current_causal_mask = causal_mask[i:i+1, :i+1]\n",
    "        output_step, past_key_value = mha(current_input, causal_mask = current_causal_mask, past_key_value = past_key_value, use_cache = True)\n",
    "        outputs.append(output_step)\n",
    "\n",
    "    output = torch.cat(outputs, dim = 1)\n",
    "\n",
    "    print(\"Input shape:\", hidden_state.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_MHA_with_cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MQA 多查询注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def split(self, x, num_heads = None):\n",
    "        batch_size = x.size(0)\n",
    "        if num_heads is None:\n",
    "            num_heads = self.num_heads\n",
    "        \n",
    "        return x.reshape(batch_size, -1, num_heads, self.head_dim).tranpose(1, 2)\n",
    "    \n",
    "    def forward(self, hidden_state, causal_mask = None, padding_mask = None):\n",
    "        batch_size = hidden_state.size(0)\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split(query, self.num_heads)\n",
    "        key = self.split(key, 1)\n",
    "        value = self.split(value, 1)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.tranpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype= torch.float32))\n",
    "\n",
    "        if causal_mask is not None:\n",
    "            attention_scores += causal_mask * -1e9\n",
    "        \n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_scores += padding_mask * -1e9\n",
    "        \n",
    "        attention_probs = torch.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        output = torch.transpose(1, 2).view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        output = self.o_linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GQA 分组查询注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_groups):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.num_groups * self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.num_groups * self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def split(self, x, num_groups = None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        if num_groups is None:\n",
    "            return x.view(batch_size, -1, self.num_heads, self.head_dim).tranpose(1, 2)\n",
    "        else:\n",
    "            x = x.view(batch_size, -1, num_groups, self.head_dim).transpose(1, 2)\n",
    "            x = x[:, :, None, :, :].expand(batch_size, num_groups, self.num_heads // num_groups, seq_len, self.head_dim)\n",
    "            x = x.view(batch_size, -1, self.num_heads, seq_len, self.head_dim)\n",
    "            return x\n",
    "    \n",
    "    def forward(self, hidden_state, causal_mask = None, padding_mask = None):\n",
    "        batch_size = hidden_state.size(0)\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split(query, self.num_heads)\n",
    "        key = self.split(key, self.num_groups)\n",
    "        value = self.split(value, self.num_groups)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.tranpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype= torch.float32))\n",
    "\n",
    "        if causal_mask is not None:\n",
    "            attention_scores += causal_mask * -1e9\n",
    "        \n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_scores += padding_mask * -1e9\n",
    "        \n",
    "        attention_probs = torch.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        output = torch.transpose(1, 2).view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        output = self.o_linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfAttention 自注意力（单头）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 创建Q、K、V的线性变换层\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, causal_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        自注意力前向传播\n",
    "        \n",
    "        参数:\n",
    "        hidden_state: 输入张量，形状为(batch_size, seq_len, hidden_size)\n",
    "        causal_mask: 因果掩码，防止未来信息泄露\n",
    "        padding_mask: 填充掩码，忽略填充位置\n",
    "        \n",
    "        返回:\n",
    "        输出张量，形状与输入相同\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "        \n",
    "        # 计算Q、K、V\n",
    "        query = self.q_linear(hidden_state)  # (b, s, h)\n",
    "        key = self.k_linear(hidden_state)    # (b, s, h)\n",
    "        value = self.v_linear(hidden_state)  # (b, s, h)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1))  # (b, s, s)\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))\n",
    "        \n",
    "        # 应用掩码\n",
    "        if causal_mask is not None:\n",
    "            # 确保掩码形状与注意力分数匹配\n",
    "            if causal_mask.dim() == 2:\n",
    "                causal_mask = causal_mask.unsqueeze(0)  # 增加批处理维度\n",
    "            attention_scores += causal_mask * -1e9\n",
    "        \n",
    "        if padding_mask is not None:\n",
    "            # 将填充掩码扩展为与注意力分数相同的形状\n",
    "            padding_mask = padding_mask.unsqueeze(1)  # (b, 1, s)\n",
    "            padding_mask = padding_mask.expand(-1, seq_len, -1)  # (b, s, s)\n",
    "            attention_scores += padding_mask * -1e9\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)  # (b, s, s)\n",
    "        \n",
    "        # 计算上下文向量\n",
    "        context = torch.matmul(attention_probs, value)  # (b, s, h)\n",
    "        \n",
    "        # 输出变换\n",
    "        output = self.o_linear(context)  # (b, s, h)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def test_self_attention():\n",
    "    batch_size = 128\n",
    "    seq_len = 512\n",
    "    hidden_size = 1024\n",
    "    \n",
    "    # 创建输入数据\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # 创建因果掩码（防止未来信息泄露）\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    \n",
    "    # 创建填充掩码示例（假设前10个位置是填充的）\n",
    "    padding_mask = torch.zeros(batch_size, seq_len).bool()\n",
    "    padding_mask[:, :10] = True\n",
    "    \n",
    "    # 初始化自注意力层\n",
    "    sa = SelfAttention(hidden_size)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = sa(hidden_state, causal_mask=causal_mask, padding_mask=padding_mask)\n",
    "    \n",
    "    print(\"Input shape:\", hidden_state.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output requires_grad:\", output.requires_grad)  # 应返回True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_self_attention()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
